{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
        "!unzip -q tiny-imagenet-200.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDbIaDa_D5HA",
        "outputId": "2f8a309f-329a-4da4-dce4-3eab6dbf7a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-15 08:34:01--  http://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Resolving cs231n.stanford.edu (cs231n.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://cs231n.stanford.edu/tiny-imagenet-200.zip [following]\n",
            "--2025-08-15 08:34:01--  https://cs231n.stanford.edu/tiny-imagenet-200.zip\n",
            "Connecting to cs231n.stanford.edu (cs231n.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 248100043 (237M) [application/zip]\n",
            "Saving to: ‘tiny-imagenet-200.zip’\n",
            "\n",
            "tiny-imagenet-200.z 100%[===================>] 236.61M  30.9MB/s    in 6.3s    \n",
            "\n",
            "2025-08-15 08:34:07 (37.3 MB/s) - ‘tiny-imagenet-200.zip’ saved [248100043/248100043]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, time\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, utils"
      ],
      "metadata": {
        "id": "SR0A9gtULloh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    HAS_SK = True\n",
        "except Exception:\n",
        "    HAS_SK = False"
      ],
      "metadata": {
        "id": "6UqjPt2HLn3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the variable"
      ],
      "metadata": {
        "id": "u9ZxL9G7Oo9X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "SEED = 42\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "AMP = True\n",
        "\n",
        "\n",
        "TRAIN_DIR = \"/content/tiny-imagenet-200/train\"\n",
        "\n",
        "NUM_CLASSES = 20\n",
        "IMG_SIZE = 64\n",
        "VAL_SPLIT = 0.15\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 0.05\n",
        "EARLY_STOP = 6\n",
        "\n",
        "OUT_DIR = Path(\"tri_compare_vit\")\n",
        "(OUT_DIR / \"figs\").mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"grids\").mkdir(parents=True, exist_ok=True)\n",
        "(OUT_DIR / \"curves\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "id": "WN4r7d8NLp9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data & selection of 20 classes"
      ],
      "metadata": {
        "id": "x6-xvP7sO2Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4802,0.4481,0.3975), std=(0.2770,0.2691,0.2821)),\n",
        "])\n",
        "\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.4802,0.4481,0.3975), std=(0.2770,0.2691,0.2821)),\n",
        "])"
      ],
      "metadata": {
        "id": "UFCKFeweLwPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = datasets.ImageFolder(TRAIN_DIR)\n",
        "all_classes = full_ds.classes\n",
        "print(f\"Found {len(all_classes)} classes; selecting {NUM_CLASSES}…\")\n",
        "rng = random.Random(SEED)\n",
        "selected_classes = sorted(rng.sample(all_classes, NUM_CLASSES))\n",
        "print(\"Selected classes:\", selected_classes)\n",
        "\n",
        "sel_to_new = {c:i for i,c in enumerate(selected_classes)}\n",
        "\n",
        "sel_indices = [i for i,(_,y) in enumerate(full_ds.samples) if full_ds.classes[y] in sel_to_new]"
      ],
      "metadata": {
        "id": "y5SYXnibLyzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WrappedSubset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, indices, transform, sel_to_new):\n",
        "        self.base = base\n",
        "        self.indices = indices\n",
        "        self.transform = transform\n",
        "        self.sel_to_new = sel_to_new\n",
        "\n",
        "    def __len__(self): return len(self.indices)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        bi = self.indices[i]\n",
        "        path, y_old = self.base.samples[bi]\n",
        "        img = self.base.loader(path)\n",
        "        img = self.transform(img)\n",
        "        y = self.sel_to_new[self.base.classes[y_old]]\n",
        "        return img, y"
      ],
      "metadata": {
        "id": "UQzWli8TL2fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "N = len(sel_indices)\n",
        "n_val = int(round(VAL_SPLIT * N))\n",
        "n_train = N - n_val\n",
        "train_ids, val_ids = random_split(sel_indices, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
        "\n",
        "train_ds = WrappedSubset(full_ds, list(train_ids), transform=train_tfms, sel_to_new=sel_to_new)\n",
        "val_ds   = WrappedSubset(full_ds, list(val_ids),  transform=eval_tfms,  sel_to_new=sel_to_new)\n",
        "\n",
        "num_workers = min(8, os.cpu_count() or 2)\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)"
      ],
      "metadata": {
        "id": "1ZFtH14ML45A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### print image for each class"
      ],
      "metadata": {
        "id": "eHEBimkgPUSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def denorm(x):\n",
        "    mean = torch.tensor([0.4802, 0.4481, 0.3975])[:,None,None]\n",
        "    std  = torch.tensor([0.2770, 0.2691, 0.2821])[:,None,None]\n",
        "    return x*std + mean\n",
        "\n",
        "def save_example_grids_per_class(dataset, per_class=20):\n",
        "    per = {i:[] for i in range(NUM_CLASSES)}\n",
        "    for i in range(len(dataset)):\n",
        "        _, y = dataset[i]\n",
        "        if len(per[y]) < per_class:\n",
        "            per[y].append(i)\n",
        "    inv_map = {v:k for k,v in sel_to_new.items()}\n",
        "    for ci, idxs in per.items():\n",
        "        imgs = [dataset[j][0] for j in idxs]\n",
        "        if not imgs: continue\n",
        "        grid = utils.make_grid(imgs, nrow=5, padding=2)\n",
        "        fig = plt.figure(figsize=(6,5))\n",
        "        plt.imshow(np.transpose(denorm(grid).clamp(0,1).numpy(), (1,2,0)))\n",
        "        plt.axis(\"off\"); plt.title(f\"class {ci}: {inv_map[ci]}\")\n",
        "        p = OUT_DIR / \"grids\" / f\"class_{ci:02d}.png\"\n",
        "        fig.tight_layout(); fig.savefig(p, dpi=200); plt.close(fig)\n",
        "        print(\"Saved:\", p)\n",
        "\n",
        "print(\"Saving a 20-image grid for each class from the training set…\")\n",
        "save_example_grids_per_class(train_ds, per_class=20)"
      ],
      "metadata": {
        "id": "U2Cf9kspL-VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_one_per_class_grid(dataset, out_path, classes=NUM_CLASSES, nrow=5):\n",
        "    found = {c: None for c in range(classes)}\n",
        "    images, labels = [], []\n",
        "    for i in range(len(dataset)):\n",
        "        x, y = dataset[i]\n",
        "        if found[y] is None:\n",
        "            found[y] = i\n",
        "            images.append(x); labels.append(y)\n",
        "        if len(images) == classes: break\n",
        "    if len(images) < classes:\n",
        "        need = [c for c, idx in found.items() if idx is None]\n",
        "        for j in range(len(val_ds)):\n",
        "            x, y = val_ds[j]\n",
        "            if y in need and all(l != y for l in labels):\n",
        "                images.append(x); labels.append(y)\n",
        "            if len(images) == classes: break\n",
        "    assert len(images) == classes, \"Could not find one sample for each class.\"\n",
        "    grid = utils.make_grid(images, nrow=nrow, padding=2)\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(np.transpose(denorm(grid).clamp(0,1).numpy(), (1,2,0)))\n",
        "    plt.axis(\"off\"); plt.title(\"One image per class (20 classes)\")\n",
        "    fig.tight_layout(); fig.savefig(out_path, dpi=220); plt.close(fig)\n",
        "    print(\"Saved:\", out_path)\n",
        "\n",
        "one_per_class_path = OUT_DIR / \"figs\" / \"one_per_class_20_grid.png\"\n",
        "save_one_per_class_grid(train_ds, one_per_class_path, classes=NUM_CLASSES, nrow=5)"
      ],
      "metadata": {
        "id": "zr-afHNaMCKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FCNN"
      ],
      "metadata": {
        "id": "B2LTyHIbMKXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class FCFNN(nn.Module):\n",
        "    def __init__(self, img_size=64, num_classes=20, widths=(1024,512)):\n",
        "        super().__init__()\n",
        "        C=3; H=W=img_size\n",
        "        flat = C*H*W\n",
        "        layers = [nn.Flatten()]\n",
        "        in_dim = flat\n",
        "        for w in widths:\n",
        "            layers += [nn.Linear(in_dim, w), nn.ReLU(inplace=True), nn.Dropout(0.2)]\n",
        "            in_dim = w\n",
        "        layers += [nn.Linear(in_dim, num_classes)]\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self, x): return self.net(x)"
      ],
      "metadata": {
        "id": "38aJmYj-MIfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small CNN\n"
      ],
      "metadata": {
        "id": "NTF3jGtLMPfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self, num_classes=20):\n",
        "        super().__init__()\n",
        "        def block(cin, cout):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(cin, cout, 3, padding=1), nn.BatchNorm2d(cout), nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(cout, cout, 3, padding=1), nn.BatchNorm2d(cout), nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2)\n",
        "            )\n",
        "        self.features = nn.Sequential(\n",
        "            block(3,   64),\n",
        "            block(64, 128),\n",
        "            block(128,256),\n",
        "        )\n",
        "        self.head = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "wO02PyLQMR2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All patchs"
      ],
      "metadata": {
        "id": "sOs01LxeMYsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Module):\n",
        "    def __init__(self, dim, max_len=1024):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        L = x.size(1)\n",
        "        return x + self.pe[:, :L, :]\n",
        "\n",
        "class PatchifyConv(nn.Module):\n",
        "    def __init__(self, img_size=64, patch=8, in_ch=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        assert img_size % patch == 0\n",
        "        self.num_patches = (img_size // patch) ** 2\n",
        "        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch, stride=patch)\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class PatchifyLinear(nn.Module):\n",
        "    def __init__(self, img_size=64, patch=8, in_ch=3, embed_dim=192):\n",
        "        super().__init__()\n",
        "        assert img_size % patch == 0\n",
        "        self.patch = patch\n",
        "        self.num_patches = (img_size // patch) ** 2\n",
        "        self.proj = nn.Linear(in_ch*patch*patch, embed_dim)\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        p = self.patch\n",
        "        patches = F.unfold(x, kernel_size=p, stride=p)\n",
        "        patches = patches.transpose(1,2)\n",
        "        return self.proj(patches)"
      ],
      "metadata": {
        "id": "66kUg8neMbw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "E0XtcOcyMjJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads, mlp_ratio=4.0, attn_drop=0.0, proj_drop=0.0, drop=0.0):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=attn_drop, batch_first=True)\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp   = nn.Sequential(\n",
        "            nn.Linear(dim, int(dim*mlp_ratio)),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(proj_drop),\n",
        "            nn.Linear(int(dim*mlp_ratio), dim),\n",
        "            nn.Dropout(proj_drop),\n",
        "        )\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.norm1(x)\n",
        "        x,_ = self.attn(x,x,x, need_weights=False)\n",
        "        x = h + self.drop1(x)\n",
        "        h = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = h + self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=64, patch=8, in_ch=3, num_classes=20,\n",
        "                 embed_dim=192, depth=8, heads=6, mlp_ratio=4.0,\n",
        "                 patch_type=\"conv\",      # \"conv\" | \"linear\"\n",
        "                 pos_type=\"learnable\"    # \"learnable\" | \"sinusoidal\" | \"none\"\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        if patch_type == \"conv\":\n",
        "            self.patch = PatchifyConv(img_size, patch, in_ch, embed_dim)\n",
        "        elif patch_type == \"linear\":\n",
        "            self.patch = PatchifyLinear(img_size, patch, in_ch, embed_dim)\n",
        "        else:\n",
        "            raise ValueError(\"patch_type must be 'conv' or 'linear'.\")\n",
        "\n",
        "        num_patches = self.patch.num_patches\n",
        "        self.cls = nn.Parameter(torch.zeros(1,1,embed_dim))\n",
        "        self.pos_type = pos_type\n",
        "        if pos_type == \"learnable\":\n",
        "            self.pos = nn.Parameter(torch.zeros(1, 1+num_patches, embed_dim))\n",
        "            nn.init.trunc_normal_(self.pos, std=0.02)\n",
        "        elif pos_type == \"sinusoidal\":\n",
        "            self.pos = SinusoidalPositionalEmbedding(embed_dim, max_len=1+num_patches)\n",
        "        elif pos_type == \"none\":\n",
        "            self.pos = None\n",
        "        else:\n",
        "            raise ValueError(\"pos_type must be learnable/sinusoidal/none\")\n",
        "\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, heads, mlp_ratio) for _ in range(depth)])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "        nn.init.trunc_normal_(self.cls, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch(x)                      # (B, N, E)\n",
        "        cls = self.cls.expand(B,-1,-1)         # (B,1,E)\n",
        "        x = torch.cat([cls, x], dim=1)         # (B,1+N,E)\n",
        "        if self.pos_type == \"learnable\":\n",
        "            x = x + self.pos[:, :x.size(1), :]\n",
        "        elif self.pos_type == \"sinusoidal\":\n",
        "            x = self.pos(x)\n",
        "        for blk in self.blocks: x = blk(x)\n",
        "        x = self.norm(x)[:,0]\n",
        "        return self.head(x)"
      ],
      "metadata": {
        "id": "KRyApnTdMlmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Evaluation"
      ],
      "metadata": {
        "id": "aPmQl6ggMoD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top1(logits, y):\n",
        "    return (logits.argmax(1) == y).float().mean().item()\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    tot_loss=tot_acc=0.0; n=0\n",
        "    all_logits=[]; all_y=[]\n",
        "    for x,y in loader:\n",
        "        x=x.to(device, non_blocking=True); y=y.to(device, non_blocking=True)\n",
        "        logits = model(x)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        bs = x.size(0)\n",
        "        tot_loss += loss.item()*bs\n",
        "        tot_acc  += top1(logits, y)*bs\n",
        "        n += bs\n",
        "        all_logits.append(logits.cpu()); all_y.append(y.cpu())\n",
        "    return tot_loss/n, tot_acc/n, torch.cat(all_logits), torch.cat(all_y)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, wd=WEIGHT_DECAY, device=DEVICE, early_stop=EARLY_STOP, run_name=\"run\"):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
        "\n",
        "    history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "    best = float(\"inf\"); best_state=None; wait=0\n",
        "\n",
        "    for ep in range(1, epochs+1):\n",
        "        model.train()\n",
        "        tr_loss=tr_acc=0.0; n=0\n",
        "        t0=time.time()\n",
        "        for x,y in train_loader:\n",
        "            x=x.to(device, non_blocking=True); y=y.to(device, non_blocking=True)\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n",
        "                logits = model(x)\n",
        "                loss = F.cross_entropy(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            scaler.step(opt); scaler.update()\n",
        "\n",
        "            bs=x.size(0)\n",
        "            tr_loss += loss.item()*bs\n",
        "            tr_acc  += top1(logits, y)*bs\n",
        "            n += bs\n",
        "\n",
        "        tr_loss/=n; tr_acc/=n\n",
        "        val_loss, val_acc, _, _ = evaluate(model, val_loader, device)\n",
        "        sch.step()\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"[{run_name}] Epoch {ep:02d}: \"\n",
        "              f\"train_loss={tr_loss:.4f} acc={tr_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"lr={sch.get_last_lr()[0]:.2e} time={time.time()-t0:.1f}s\")\n",
        "\n",
        "        if val_loss < best - 1e-4:\n",
        "            best = val_loss\n",
        "            best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
        "            wait=0\n",
        "        else:\n",
        "            wait+=1\n",
        "            if wait>=early_stop:\n",
        "                print(f\"[{run_name}] Early stop.\")\n",
        "                break\n",
        "\n",
        "    if best_state is not None: model.load_state_dict(best_state)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "PTKimvnuMycg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot curves"
      ],
      "metadata": {
        "id": "d0pxkCNAM0AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(history, title, out_png):\n",
        "    ep = range(1, len(history[\"train_loss\"])+1)\n",
        "    plt.figure(figsize=(7.5,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(ep, history[\"train_loss\"], label=\"Train\")\n",
        "    plt.plot(ep, history[\"val_loss\"],   label=\"Val\")\n",
        "    plt.title(\"Loss\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(ep, history[\"train_acc\"], label=\"Train\")\n",
        "    plt.plot(ep, history[\"val_acc\"],   label=\"Val\")\n",
        "    plt.title(\"Accuracy\"); plt.xlabel(\"Epoch\"); plt.ylabel(\"Acc\"); plt.legend()\n",
        "    plt.suptitle(title); plt.tight_layout()\n",
        "    plt.savefig(out_png, dpi=220); plt.close()\n",
        "    print(\"Saved:\", out_png)"
      ],
      "metadata": {
        "id": "Aurz2uK4M3Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment Registry"
      ],
      "metadata": {
        "id": "jt9fQMpMNBGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class RunSpec:\n",
        "    family: str         # \"ViT\" | \"CNN\" | \"FCFNN\"\n",
        "    name: str           # short name used in plots\n",
        "    params: dict        # kwargs for model ctor\n",
        "\n",
        "def build_model(spec: RunSpec):\n",
        "    if spec.family == \"ViT\":\n",
        "        return ViT(num_classes=NUM_CLASSES, img_size=IMG_SIZE, **spec.params)\n",
        "    if spec.family == \"CNN\":\n",
        "        return SmallCNN(num_classes=NUM_CLASSES)\n",
        "    if spec.family == \"FCFNN\":\n",
        "        return FCFNN(img_size=IMG_SIZE, num_classes=NUM_CLASSES, **spec.params)\n",
        "    raise ValueError(\"Unknown family\")\n",
        "\n",
        "# Baselines\n",
        "runs = [\n",
        "    RunSpec(\"FCFNN\", \"FCFNN\", {\"widths\": (1024,512)}),\n",
        "    RunSpec(\"CNN\",   \"SmallCNN\", {}),\n",
        "    RunSpec(\"ViT\",   \"ViT(h=4,Conv,LearnPE)\", {\"patch\":8, \"embed_dim\":192, \"depth\":8, \"heads\":4, \"patch_type\":\"conv\", \"pos_type\":\"learnable\"}),\n",
        "]\n",
        "\n",
        "# ViT ablation: number of heads\n",
        "for h in [2,4,6,8]:\n",
        "    runs.append(RunSpec(\"ViT\", f\"ViT_heads={h}\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":h,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}))\n",
        "\n",
        "# ViT ablation: patch embedding type\n",
        "runs += [\n",
        "    RunSpec(\"ViT\", \"ViT_PatchConv\",   {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PatchLinear\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"linear\",\"pos_type\":\"learnable\"}),\n",
        "]\n",
        "\n",
        "# ViT ablation: positional embedding type\n",
        "runs += [\n",
        "    RunSpec(\"ViT\", \"ViT_PosLearn\", {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"learnable\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PosSine\",  {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"sinusoidal\"}),\n",
        "    RunSpec(\"ViT\", \"ViT_PosNone\",  {\"patch\":8,\"embed_dim\":192,\"depth\":6,\"heads\":4,\"patch_type\":\"conv\",\"pos_type\":\"none\"}),\n",
        "]"
      ],
      "metadata": {
        "id": "IDadDKRiNDtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run all experiments"
      ],
      "metadata": {
        "id": "4igh5rtPNQQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {}\n",
        "results = []\n",
        "\n",
        "for spec in runs:\n",
        "    run_name = spec.name\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Training:\", run_name, \"|\", spec.family, spec.params)\n",
        "    model = build_model(spec)\n",
        "    model, hist = train_model(model, train_loader, val_loader, run_name=run_name)\n",
        "    histories[run_name] = hist\n",
        "\n",
        "    curve_path = OUT_DIR / \"curves\" / f\"{run_name}.png\"\n",
        "    plot_curves(hist, title=run_name, out_png=curve_path)\n",
        "\n",
        "    val_loss, val_acc, logits, tgts = evaluate(model, val_loader, DEVICE)\n",
        "    rec = {\"name\": run_name, \"family\": spec.family, \"val_loss\": val_loss, \"val_acc\": val_acc}\n",
        "    rec.update({f\"p:{k}\":v for k,v in spec.params.items()})\n",
        "    results.append(rec)\n",
        "\n",
        "    if HAS_SK and spec.family in [\"ViT\",\"CNN\"]:\n",
        "        cm = confusion_matrix(tgts.numpy(), logits.argmax(1).numpy(), labels=list(range(NUM_CLASSES)))\n",
        "        plt.figure(figsize=(6,5))\n",
        "        plt.imshow(cm, interpolation='nearest'); plt.colorbar()\n",
        "        plt.title(f\"Confusion Matrix — {run_name}\")\n",
        "        plt.xlabel(\"Pred\"); plt.ylabel(\"True\"); plt.tight_layout()\n",
        "        p = OUT_DIR / \"figs\" / f\"cm_{run_name}.png\"\n",
        "        plt.savefig(p, dpi=200); plt.close()\n",
        "        print(\"Saved:\", p)"
      ],
      "metadata": {
        "id": "n4yJ1zRwNYgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summaries & Standard Comparison Plots"
      ],
      "metadata": {
        "id": "1sd_2U5KNo_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "csv_path = OUT_DIR / \"summary.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(\"\\nSaved summary:\", csv_path)\n",
        "print(df.sort_values(\"val_acc\", ascending=False)[[\"name\",\"family\",\"val_acc\",\"val_loss\"]].to_string(index=False))\n",
        "\n",
        "best_per_family = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "plt.figure(figsize=(7,4))\n",
        "plt.bar(best_per_family[\"family\"], best_per_family[\"val_acc\"])\n",
        "for x, a in zip(best_per_family[\"family\"], best_per_family[\"val_acc\"]):\n",
        "    plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "plt.title(\"Best Validation Accuracy per Family\"); plt.ylabel(\"Val Acc\"); plt.tight_layout()\n",
        "p = OUT_DIR / \"figs\" / \"best_per_family.png\"\n",
        "plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_heads = df[df[\"name\"].str.startswith(\"ViT_heads\")]\n",
        "if not vit_heads.empty:\n",
        "    vit_heads_sorted = vit_heads.sort_values(\"p:heads\")\n",
        "    plt.figure(figsize=(7,4))\n",
        "    xs = [str(h) for h in vit_heads_sorted[\"p:heads\"]]\n",
        "    plt.bar(xs, vit_heads_sorted[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_heads_sorted[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.xlabel(\"#Heads\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.title(\"Effect of Number of Heads (ViT)\"); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_heads_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_patch = df[df[\"name\"].str.startswith(\"ViT_Patch\")]\n",
        "if len(vit_patch) >= 2:\n",
        "    vit_patch = vit_patch.sort_values(\"name\")\n",
        "    plt.figure(figsize=(7,4))\n",
        "    xs = vit_patch[\"name\"].tolist()\n",
        "    plt.bar(xs, vit_patch[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_patch[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.title(\"Effect of Patch Embedding (ViT)\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.xticks(rotation=15); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_patch_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)\n",
        "\n",
        "vit_pos = df[df[\"name\"].str.startswith(\"ViT_Pos\")]\n",
        "if len(vit_pos) >= 2:\n",
        "    vit_pos = vit_pos.sort_values(\"name\")\n",
        "    plt.figure(figsize=(7.5,4))\n",
        "    xs = vit_pos[\"name\"].tolist()\n",
        "    plt.bar(xs, vit_pos[\"val_acc\"])\n",
        "    for x,a in zip(xs, vit_pos[\"val_acc\"]):\n",
        "        plt.text(x, a+0.002, f\"{a:.3f}\", ha=\"center\", va=\"bottom\")\n",
        "    plt.title(\"Effect of Positional Embedding (ViT)\"); plt.ylabel(\"Val Acc\")\n",
        "    plt.xticks(rotation=15); plt.tight_layout()\n",
        "    p = OUT_DIR / \"figs\" / \"vit_pos_ablation.png\"\n",
        "    plt.savefig(p, dpi=220); plt.close(); print(\"Saved:\", p)"
      ],
      "metadata": {
        "id": "WJQvrXioNuvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###COMBINED TABLE + DASHBOARD + COMBINED CURVES + PDF REPORT\n"
      ],
      "metadata": {
        "id": "zGC5TYE9NziY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from matplotlib.backends.backend_pdf import PdfPages\n",
        "# import matplotlib.image as mpimg\n",
        "\n",
        "# FIG_DIR = OUT_DIR / \"figs\"\n",
        "# FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# def save_combined_table(df, out_png):\n",
        "#     display_df = df.copy()\n",
        "#     display_df[\"val_acc\"] = display_df[\"val_acc\"].map(lambda v: f\"{v:.3f}\")\n",
        "#     display_df[\"val_loss\"] = display_df[\"val_loss\"].map(lambda v: f\"{v:.3f}\")\n",
        "#     for col in [\"p:heads\", \"p:patch_type\", \"p:pos_type\", \"p:embed_dim\", \"p:depth\", \"p:patch\"]:\n",
        "#         if col not in display_df.columns:\n",
        "#             display_df[col] = \"\"\n",
        "#     tmp = df.copy()\n",
        "#     tmp[\"__ord\"] = -tmp[\"val_acc\"]\n",
        "#     ord_idx = tmp.sort_values([\"family\",\"__ord\"]).index\n",
        "#     display_df = display_df.loc[ord_idx]\n",
        "#     show_cols = [\"name\",\"family\",\"val_acc\",\"val_loss\",\"p:heads\",\"p:patch_type\",\"p:pos_type\",\"p:patch\",\"p:embed_dim\",\"p:depth\"]\n",
        "#     display_df = display_df[show_cols].rename(columns={\n",
        "#         \"name\":\"Run\",\"family\":\"Family\",\"val_acc\":\"Val Acc\",\"val_loss\":\"Val Loss\",\n",
        "#         \"p:heads\":\"Heads\",\"p:patch_type\":\"Patch Emb\",\"p:pos_type\":\"Pos Emb\",\n",
        "#         \"p:patch\":\"PatchSz\",\"p:embed_dim\":\"Embed\",\"p:depth\":\"Depth\",\n",
        "#     })\n",
        "#     rows = len(display_df)\n",
        "#     fig_h = 1.0 + rows * 0.38\n",
        "#     fig, ax = plt.subplots(figsize=(13, fig_h))\n",
        "#     ax.axis(\"off\")\n",
        "#     the_table = ax.table(\n",
        "#         cellText=display_df.values,\n",
        "#         colLabels=display_df.columns.tolist(),\n",
        "#         cellLoc=\"center\",\n",
        "#         loc=\"upper left\",\n",
        "#     )\n",
        "#     the_table.auto_set_font_size(False)\n",
        "#     the_table.set_fontsize(9)\n",
        "#     the_table.scale(1, 1.2)\n",
        "#     ax.set_title(\"All Runs — Validation Results & Key Params\", pad=10, fontsize=12, weight=\"bold\")\n",
        "#     fig.tight_layout()\n",
        "#     fig.savefig(out_png, dpi=240, bbox_inches=\"tight\")\n",
        "#     plt.close(fig)\n",
        "#     print(\"Saved:\", out_png)\n",
        "\n",
        "# def save_dashboard(df, out_png):\n",
        "#     fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "#     best_per_family = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "#     axs[0,0].bar(best_per_family[\"family\"], best_per_family[\"val_acc\"])\n",
        "#     for x, a in zip(best_per_family[\"family\"], best_per_family[\"val_acc\"]):\n",
        "#         axs[0,0].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "#     axs[0,0].set_title(\"Best Validation Accuracy per Family\"); axs[0,0].set_ylabel(\"Val Acc\")\n",
        "#     axs[0,0].set_ylim(0, max(0.01 + best_per_family[\"val_acc\"].max(), 0.1))\n",
        "\n",
        "#     vit_heads = df[df[\"name\"].str.startswith(\"ViT_heads\")]\n",
        "#     axs[0,1].set_title(\"ViT — Effect of Number of Heads\")\n",
        "#     if not vit_heads.empty:\n",
        "#         vit_heads = vit_heads.sort_values(\"p:heads\")\n",
        "#         xs = vit_heads[\"p:heads\"].astype(int).tolist()\n",
        "#         axs[0,1].bar([str(x) for x in xs], vit_heads[\"val_acc\"])\n",
        "#         for x, a in zip(xs, vit_heads[\"val_acc\"]):\n",
        "#             axs[0,1].text(str(x), a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "#         axs[0,1].set_xlabel(\"#Heads\"); axs[0,1].set_ylabel(\"Val Acc\")\n",
        "#     else:\n",
        "#         axs[0,1].text(0.5, 0.5, \"No heads sweep runs\", ha=\"center\", va=\"center\")\n",
        "#         axs[0,1].set_xticks([]); axs[0,1].set_yticks([])\n",
        "\n",
        "#     vit_patch = df[df[\"name\"].str.startswith(\"ViT_Patch\")].sort_values(\"name\")\n",
        "#     axs[1,0].set_title(\"ViT — Patch Embedding Choice\")\n",
        "#     if len(vit_patch) >= 1:\n",
        "#         xs = vit_patch[\"name\"].tolist()\n",
        "#         axs[1,0].bar(xs, vit_patch[\"val_acc\"])\n",
        "#         for x, a in zip(xs, vit_patch[\"val_acc\"]):\n",
        "#             axs[1,0].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "#         axs[1,0].set_ylabel(\"Val Acc\"); axs[1,0].tick_params(axis=\"x\", rotation=15)\n",
        "#     else:\n",
        "#         axs[1,0].text(0.5, 0.5, \"No patch ablation runs\", ha=\"center\", va=\"center\")\n",
        "#         axs[1,0].set_xticks([]); axs[1,0].set_yticks([])\n",
        "\n",
        "#     vit_pos = df[df[\"name\"].str.startswith(\"ViT_Pos\")].sort_values(\"name\")\n",
        "#     axs[1,1].set_title(\"ViT — Positional Embedding Choice\")\n",
        "#     if len(vit_pos) >= 1:\n",
        "#         xs = vit_pos[\"name\"].tolist()\n",
        "#         axs[1,1].bar(xs, vit_pos[\"val_acc\"])\n",
        "#         for x, a in zip(xs, vit_pos[\"val_acc\"]):\n",
        "#             axs[1,1].text(x, a+0.003, f\"{a:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
        "#         axs[1,1].set_ylabel(\"Val Acc\"); axs[1,1].tick_params(axis=\"x\", rotation=15)\n",
        "#     else:\n",
        "#         axs[1,1].text(0.5, 0.5, \"No positional ablation runs\", ha=\"center\", va=\"center\")\n",
        "#         axs[1,1].set_xticks([]); axs[1,1].set_yticks([])\n",
        "#     fig.tight_layout(); fig.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(fig)\n",
        "#     print(\"Saved:\", out_png)\n",
        "\n",
        "# def plot_best_family_curves(df, histories_dict, out_png):\n",
        "#     plt.figure(figsize=(10, 4.5))\n",
        "#     ax1 = plt.subplot(1,2,1); ax1.set_title(\"Loss Curves — Best per Family\")\n",
        "#     ax2 = plt.subplot(1,2,2); ax2.set_title(\"Accuracy Curves — Best per Family\")\n",
        "#     fam_best = df.sort_values(\"val_acc\", ascending=False).groupby(\"family\").head(1)\n",
        "#     colors = {\"FCFNN\":\"tab:orange\", \"CNN\":\"tab:green\", \"ViT\":\"tab:blue\"}\n",
        "#     plotted = False\n",
        "#     for _, row in fam_best.iterrows():\n",
        "#         name = row[\"name\"]; fam = row[\"family\"]\n",
        "#         if name not in histories_dict:\n",
        "#             continue\n",
        "#         H = histories_dict[name]; ep = range(1, len(H[\"train_loss\"])+1); c = colors.get(fam, None)\n",
        "#         ax1.plot(ep, H[\"train_loss\"], label=f\"{fam}-{name} (train)\", linestyle=\"--\", color=c)\n",
        "#         ax1.plot(ep, H[\"val_loss\"],   label=f\"{fam}-{name} (val)\",   linestyle=\"-\",  color=c)\n",
        "#         ax2.plot(ep, H[\"train_acc\"],  label=f\"{fam}-{name} (train)\", linestyle=\"--\", color=c)\n",
        "#         ax2.plot(ep, H[\"val_acc\"],    label=f\"{fam}-{name} (val)\",   linestyle=\"-\",  color=c)\n",
        "#         plotted = True\n",
        "#     for ax in (ax1, ax2):\n",
        "#         ax.set_xlabel(\"Epoch\"); ax.grid(True, linestyle=\"--\", linewidth=0.5); ax.legend(fontsize=8)\n",
        "#     if not plotted:\n",
        "#         plt.clf()\n",
        "#         fig = plt.figure(figsize=(6,2)); plt.axis(\"off\")\n",
        "#         plt.text(0.5, 0.5, \"Histories not available\", ha=\"center\", va=\"center\")\n",
        "#         fig.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(fig); print(\"Saved:\", out_png);\n",
        "#         return\n",
        "#     plt.tight_layout(); plt.savefig(out_png, dpi=240, bbox_inches=\"tight\"); plt.close(); print(\"Saved:\", out_png)\n",
        "\n",
        "# combined_table_png = FIG_DIR / \"combined_table.png\"\n",
        "# dashboard_png      = FIG_DIR / \"dashboard.png\"\n",
        "# best_curves_png    = FIG_DIR / \"best_family_curves.png\"\n",
        "\n",
        "# save_combined_table(df, combined_table_png)\n",
        "# save_dashboard(df, dashboard_png)\n",
        "# plot_best_family_curves(df, histories, best_curves_png)\n",
        "\n",
        "# pdf_path = FIG_DIR / \"report.pdf\"\n",
        "# with PdfPages(pdf_path) as pdf:\n",
        "#     for img_path in [combined_table_png, dashboard_png, best_curves_png]:\n",
        "#         if os.path.exists(img_path):\n",
        "#             fig = plt.figure(figsize=(11, 8.5))\n",
        "#             plt.imshow(mpimg.imread(img_path)); plt.axis(\"off\")\n",
        "#             pdf.savefig(fig, bbox_inches=\"tight\"); plt.close(fig)\n",
        "# print(\"Saved PDF report:\", pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fm9XTHa_YNC3",
        "outputId": "24d4213c-a488-41ce-98b9-37052737e994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 classes; selecting 20…\n",
            "Selected classes: ['n01768244', 'n01770393', 'n01774384', 'n02058221', 'n02074367', 'n02099601', 'n02106662', 'n02132136', 'n02481823', 'n02504458', 'n02666196', 'n02730930', 'n02814533', 'n03447447', 'n04067472', 'n04265275', 'n04456115', 'n04562935', 'n07753592', 'n07768694']\n",
            "Saving a 20-image grid for each class from the training set…\n",
            "Saved: tri_compare_vit/grids/class_00.png\n",
            "Saved: tri_compare_vit/grids/class_01.png\n",
            "Saved: tri_compare_vit/grids/class_02.png\n",
            "Saved: tri_compare_vit/grids/class_03.png\n",
            "Saved: tri_compare_vit/grids/class_04.png\n",
            "Saved: tri_compare_vit/grids/class_05.png\n",
            "Saved: tri_compare_vit/grids/class_06.png\n",
            "Saved: tri_compare_vit/grids/class_07.png\n",
            "Saved: tri_compare_vit/grids/class_08.png\n",
            "Saved: tri_compare_vit/grids/class_09.png\n",
            "Saved: tri_compare_vit/grids/class_10.png\n",
            "Saved: tri_compare_vit/grids/class_11.png\n",
            "Saved: tri_compare_vit/grids/class_12.png\n",
            "Saved: tri_compare_vit/grids/class_13.png\n",
            "Saved: tri_compare_vit/grids/class_14.png\n",
            "Saved: tri_compare_vit/grids/class_15.png\n",
            "Saved: tri_compare_vit/grids/class_16.png\n",
            "Saved: tri_compare_vit/grids/class_17.png\n",
            "Saved: tri_compare_vit/grids/class_18.png\n",
            "Saved: tri_compare_vit/grids/class_19.png\n",
            "Saved: tri_compare_vit/figs/one_per_class_20_grid.png\n",
            "\n",
            "================================================================================\n",
            "Training: FCFNN | FCFNN {'widths': (1024, 512)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FCFNN] Epoch 01: train_loss=2.6309 acc=0.1984 | val_loss=2.4172 acc=0.2773 | lr=2.98e-04 time=11.7s\n",
            "[FCFNN] Epoch 02: train_loss=2.4121 acc=0.2645 | val_loss=2.3404 acc=0.2880 | lr=2.93e-04 time=10.8s\n",
            "[FCFNN] Epoch 03: train_loss=2.3172 acc=0.2992 | val_loss=2.3018 acc=0.3127 | lr=2.84e-04 time=10.1s\n",
            "[FCFNN] Epoch 04: train_loss=2.2292 acc=0.3268 | val_loss=2.2651 acc=0.3147 | lr=2.71e-04 time=10.8s\n",
            "[FCFNN] Epoch 05: train_loss=2.1331 acc=0.3572 | val_loss=2.2449 acc=0.3393 | lr=2.56e-04 time=11.9s\n",
            "[FCFNN] Epoch 06: train_loss=2.0871 acc=0.3706 | val_loss=2.2183 acc=0.3407 | lr=2.38e-04 time=13.1s\n",
            "[FCFNN] Epoch 07: train_loss=2.0074 acc=0.3958 | val_loss=2.2134 acc=0.3380 | lr=2.18e-04 time=10.9s\n",
            "[FCFNN] Epoch 08: train_loss=1.9586 acc=0.4185 | val_loss=2.1913 acc=0.3453 | lr=1.96e-04 time=11.0s\n",
            "[FCFNN] Epoch 09: train_loss=1.8815 acc=0.4387 | val_loss=2.1921 acc=0.3620 | lr=1.73e-04 time=11.1s\n",
            "[FCFNN] Epoch 10: train_loss=1.8223 acc=0.4498 | val_loss=2.1878 acc=0.3460 | lr=1.50e-04 time=11.0s\n",
            "[FCFNN] Epoch 11: train_loss=1.7786 acc=0.4740 | val_loss=2.1941 acc=0.3533 | lr=1.27e-04 time=11.3s\n",
            "[FCFNN] Epoch 12: train_loss=1.7109 acc=0.4939 | val_loss=2.1831 acc=0.3600 | lr=1.04e-04 time=10.3s\n",
            "[FCFNN] Epoch 13: train_loss=1.6664 acc=0.5139 | val_loss=2.1735 acc=0.3627 | lr=8.19e-05 time=11.0s\n",
            "[FCFNN] Epoch 14: train_loss=1.6133 acc=0.5260 | val_loss=2.1725 acc=0.3720 | lr=6.18e-05 time=11.1s\n",
            "[FCFNN] Epoch 15: train_loss=1.5783 acc=0.5376 | val_loss=2.1713 acc=0.3607 | lr=4.39e-05 time=11.0s\n",
            "[FCFNN] Epoch 16: train_loss=1.5512 acc=0.5544 | val_loss=2.1662 acc=0.3613 | lr=2.86e-05 time=10.9s\n",
            "[FCFNN] Epoch 17: train_loss=1.5310 acc=0.5566 | val_loss=2.1592 acc=0.3633 | lr=1.63e-05 time=11.0s\n",
            "[FCFNN] Epoch 18: train_loss=1.5134 acc=0.5621 | val_loss=2.1613 acc=0.3687 | lr=7.34e-06 time=10.4s\n",
            "[FCFNN] Epoch 19: train_loss=1.5028 acc=0.5709 | val_loss=2.1616 acc=0.3687 | lr=1.85e-06 time=10.6s\n",
            "[FCFNN] Epoch 20: train_loss=1.4964 acc=0.5708 | val_loss=2.1616 acc=0.3693 | lr=0.00e+00 time=11.1s\n",
            "Saved: tri_compare_vit/curves/FCFNN.png\n",
            "\n",
            "================================================================================\n",
            "Training: SmallCNN | CNN {}\n",
            "[SmallCNN] Epoch 01: train_loss=2.4157 acc=0.2773 | val_loss=2.1839 acc=0.3267 | lr=2.98e-04 time=12.6s\n",
            "[SmallCNN] Epoch 02: train_loss=2.0870 acc=0.3865 | val_loss=1.9360 acc=0.4313 | lr=2.93e-04 time=12.0s\n",
            "[SmallCNN] Epoch 03: train_loss=1.9106 acc=0.4404 | val_loss=1.8341 acc=0.4500 | lr=2.84e-04 time=12.0s\n",
            "[SmallCNN] Epoch 04: train_loss=1.7968 acc=0.4748 | val_loss=1.9533 acc=0.4287 | lr=2.71e-04 time=11.9s\n",
            "[SmallCNN] Epoch 05: train_loss=1.7056 acc=0.4988 | val_loss=1.6278 acc=0.5267 | lr=2.56e-04 time=12.0s\n",
            "[SmallCNN] Epoch 06: train_loss=1.6269 acc=0.5256 | val_loss=1.5837 acc=0.5267 | lr=2.38e-04 time=11.9s\n",
            "[SmallCNN] Epoch 07: train_loss=1.5448 acc=0.5524 | val_loss=1.6456 acc=0.5000 | lr=2.18e-04 time=12.2s\n",
            "[SmallCNN] Epoch 08: train_loss=1.4743 acc=0.5826 | val_loss=1.5072 acc=0.5313 | lr=1.96e-04 time=12.1s\n",
            "[SmallCNN] Epoch 09: train_loss=1.4207 acc=0.5900 | val_loss=1.5386 acc=0.5293 | lr=1.73e-04 time=12.1s\n",
            "[SmallCNN] Epoch 10: train_loss=1.3640 acc=0.6111 | val_loss=1.4270 acc=0.5873 | lr=1.50e-04 time=12.2s\n",
            "[SmallCNN] Epoch 11: train_loss=1.3269 acc=0.6253 | val_loss=1.3683 acc=0.5847 | lr=1.27e-04 time=12.0s\n",
            "[SmallCNN] Epoch 12: train_loss=1.2823 acc=0.6371 | val_loss=1.3195 acc=0.6187 | lr=1.04e-04 time=15.7s\n",
            "[SmallCNN] Epoch 13: train_loss=1.2455 acc=0.6538 | val_loss=1.3224 acc=0.6173 | lr=8.19e-05 time=12.2s\n",
            "[SmallCNN] Epoch 14: train_loss=1.2110 acc=0.6613 | val_loss=1.2568 acc=0.6340 | lr=6.18e-05 time=12.1s\n",
            "[SmallCNN] Epoch 15: train_loss=1.1850 acc=0.6708 | val_loss=1.2186 acc=0.6493 | lr=4.39e-05 time=12.0s\n",
            "[SmallCNN] Epoch 16: train_loss=1.1662 acc=0.6814 | val_loss=1.2108 acc=0.6573 | lr=2.86e-05 time=12.1s\n",
            "[SmallCNN] Epoch 17: train_loss=1.1432 acc=0.6832 | val_loss=1.1803 acc=0.6633 | lr=1.63e-05 time=13.3s\n",
            "[SmallCNN] Epoch 18: train_loss=1.1356 acc=0.6887 | val_loss=1.1714 acc=0.6727 | lr=7.34e-06 time=12.1s\n",
            "[SmallCNN] Epoch 19: train_loss=1.1292 acc=0.6919 | val_loss=1.1648 acc=0.6700 | lr=1.85e-06 time=12.1s\n",
            "[SmallCNN] Epoch 20: train_loss=1.1102 acc=0.6978 | val_loss=1.1602 acc=0.6633 | lr=0.00e+00 time=12.1s\n",
            "Saved: tri_compare_vit/curves/SmallCNN.png\n",
            "Saved: tri_compare_vit/figs/cm_SmallCNN.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT(h=4,Conv,LearnPE) | ViT {'patch': 8, 'embed_dim': 192, 'depth': 8, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT(h=4,Conv,LearnPE)] Epoch 01: train_loss=2.6740 acc=0.1709 | val_loss=2.4760 acc=0.2247 | lr=2.98e-04 time=12.5s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 02: train_loss=2.4714 acc=0.2342 | val_loss=2.3210 acc=0.2867 | lr=2.93e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 03: train_loss=2.3021 acc=0.2840 | val_loss=2.1329 acc=0.3487 | lr=2.84e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 04: train_loss=2.1856 acc=0.3225 | val_loss=2.0623 acc=0.3813 | lr=2.71e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 05: train_loss=2.0888 acc=0.3552 | val_loss=2.0112 acc=0.3913 | lr=2.56e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 06: train_loss=1.9898 acc=0.3846 | val_loss=1.9069 acc=0.4247 | lr=2.38e-04 time=12.2s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 07: train_loss=1.8984 acc=0.4189 | val_loss=1.8694 acc=0.4360 | lr=2.18e-04 time=12.3s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 08: train_loss=1.8370 acc=0.4431 | val_loss=1.8197 acc=0.4427 | lr=1.96e-04 time=12.1s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 09: train_loss=1.7636 acc=0.4598 | val_loss=1.7904 acc=0.4653 | lr=1.73e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 10: train_loss=1.6934 acc=0.4818 | val_loss=1.7403 acc=0.4920 | lr=1.50e-04 time=12.3s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 11: train_loss=1.6362 acc=0.4998 | val_loss=1.7410 acc=0.4880 | lr=1.27e-04 time=12.1s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 12: train_loss=1.5845 acc=0.5105 | val_loss=1.6961 acc=0.4800 | lr=1.04e-04 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 13: train_loss=1.5367 acc=0.5226 | val_loss=1.6770 acc=0.5000 | lr=8.19e-05 time=12.6s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 14: train_loss=1.4866 acc=0.5376 | val_loss=1.6743 acc=0.5047 | lr=6.18e-05 time=12.6s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 15: train_loss=1.4523 acc=0.5606 | val_loss=1.6650 acc=0.5093 | lr=4.39e-05 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 16: train_loss=1.4066 acc=0.5720 | val_loss=1.6375 acc=0.5193 | lr=2.86e-05 time=12.5s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 17: train_loss=1.3738 acc=0.5832 | val_loss=1.6313 acc=0.5253 | lr=1.63e-05 time=12.4s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 18: train_loss=1.3562 acc=0.5861 | val_loss=1.6255 acc=0.5207 | lr=7.34e-06 time=12.3s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 19: train_loss=1.3517 acc=0.5912 | val_loss=1.6219 acc=0.5293 | lr=1.85e-06 time=12.3s\n",
            "[ViT(h=4,Conv,LearnPE)] Epoch 20: train_loss=1.3358 acc=0.5988 | val_loss=1.6201 acc=0.5300 | lr=0.00e+00 time=12.3s\n",
            "Saved: tri_compare_vit/curves/ViT(h=4,Conv,LearnPE).png\n",
            "Saved: tri_compare_vit/figs/cm_ViT(h=4,Conv,LearnPE).png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_heads=2 | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 2, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_heads=2] Epoch 01: train_loss=2.6449 acc=0.1822 | val_loss=2.4156 acc=0.2460 | lr=2.98e-04 time=12.2s\n",
            "[ViT_heads=2] Epoch 02: train_loss=2.3963 acc=0.2571 | val_loss=2.2147 acc=0.3360 | lr=2.93e-04 time=12.1s\n",
            "[ViT_heads=2] Epoch 03: train_loss=2.2411 acc=0.3060 | val_loss=2.0743 acc=0.3713 | lr=2.84e-04 time=12.0s\n",
            "[ViT_heads=2] Epoch 04: train_loss=2.1522 acc=0.3334 | val_loss=2.0201 acc=0.3833 | lr=2.71e-04 time=12.1s\n",
            "[ViT_heads=2] Epoch 05: train_loss=2.0646 acc=0.3638 | val_loss=1.9472 acc=0.4160 | lr=2.56e-04 time=12.0s\n",
            "[ViT_heads=2] Epoch 06: train_loss=1.9681 acc=0.3929 | val_loss=1.9158 acc=0.4153 | lr=2.38e-04 time=12.1s\n",
            "[ViT_heads=2] Epoch 07: train_loss=1.8887 acc=0.4125 | val_loss=1.8484 acc=0.4447 | lr=2.18e-04 time=12.1s\n",
            "[ViT_heads=2] Epoch 08: train_loss=1.8140 acc=0.4344 | val_loss=1.8227 acc=0.4600 | lr=1.96e-04 time=12.0s\n",
            "[ViT_heads=2] Epoch 09: train_loss=1.7586 acc=0.4555 | val_loss=1.7748 acc=0.4693 | lr=1.73e-04 time=12.3s\n",
            "[ViT_heads=2] Epoch 10: train_loss=1.6932 acc=0.4768 | val_loss=1.7240 acc=0.4780 | lr=1.50e-04 time=12.2s\n",
            "[ViT_heads=2] Epoch 11: train_loss=1.6371 acc=0.4946 | val_loss=1.7155 acc=0.4833 | lr=1.27e-04 time=12.2s\n",
            "[ViT_heads=2] Epoch 12: train_loss=1.5896 acc=0.5148 | val_loss=1.7198 acc=0.4820 | lr=1.04e-04 time=11.9s\n",
            "[ViT_heads=2] Epoch 13: train_loss=1.5466 acc=0.5253 | val_loss=1.7031 acc=0.4860 | lr=8.19e-05 time=11.6s\n",
            "[ViT_heads=2] Epoch 14: train_loss=1.5093 acc=0.5366 | val_loss=1.6646 acc=0.5080 | lr=6.18e-05 time=11.8s\n",
            "[ViT_heads=2] Epoch 15: train_loss=1.4730 acc=0.5481 | val_loss=1.6739 acc=0.5173 | lr=4.39e-05 time=11.8s\n",
            "[ViT_heads=2] Epoch 16: train_loss=1.4383 acc=0.5584 | val_loss=1.6439 acc=0.5227 | lr=2.86e-05 time=12.3s\n",
            "[ViT_heads=2] Epoch 17: train_loss=1.4138 acc=0.5713 | val_loss=1.6381 acc=0.5160 | lr=1.63e-05 time=12.1s\n",
            "[ViT_heads=2] Epoch 18: train_loss=1.3823 acc=0.5784 | val_loss=1.6374 acc=0.5187 | lr=7.34e-06 time=12.3s\n",
            "[ViT_heads=2] Epoch 19: train_loss=1.3715 acc=0.5821 | val_loss=1.6341 acc=0.5200 | lr=1.85e-06 time=12.2s\n",
            "[ViT_heads=2] Epoch 20: train_loss=1.3741 acc=0.5779 | val_loss=1.6333 acc=0.5180 | lr=0.00e+00 time=12.0s\n",
            "Saved: tri_compare_vit/curves/ViT_heads=2.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_heads=2.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_heads=4 | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_heads=4] Epoch 01: train_loss=2.6377 acc=0.1824 | val_loss=2.4178 acc=0.2513 | lr=2.98e-04 time=12.3s\n",
            "[ViT_heads=4] Epoch 02: train_loss=2.3879 acc=0.2664 | val_loss=2.1929 acc=0.3280 | lr=2.93e-04 time=12.2s\n",
            "[ViT_heads=4] Epoch 03: train_loss=2.2400 acc=0.3102 | val_loss=2.1105 acc=0.3680 | lr=2.84e-04 time=12.1s\n",
            "[ViT_heads=4] Epoch 04: train_loss=2.1253 acc=0.3420 | val_loss=2.0176 acc=0.3807 | lr=2.71e-04 time=12.2s\n",
            "[ViT_heads=4] Epoch 05: train_loss=2.0409 acc=0.3716 | val_loss=1.9768 acc=0.3927 | lr=2.56e-04 time=12.0s\n",
            "[ViT_heads=4] Epoch 06: train_loss=1.9664 acc=0.3920 | val_loss=1.9227 acc=0.4107 | lr=2.38e-04 time=12.2s\n",
            "[ViT_heads=4] Epoch 07: train_loss=1.8974 acc=0.4167 | val_loss=1.8600 acc=0.4293 | lr=2.18e-04 time=12.1s\n",
            "[ViT_heads=4] Epoch 08: train_loss=1.8065 acc=0.4391 | val_loss=1.8538 acc=0.4327 | lr=1.96e-04 time=12.1s\n",
            "[ViT_heads=4] Epoch 09: train_loss=1.7643 acc=0.4560 | val_loss=1.7796 acc=0.4593 | lr=1.73e-04 time=12.2s\n",
            "[ViT_heads=4] Epoch 10: train_loss=1.6890 acc=0.4751 | val_loss=1.8115 acc=0.4500 | lr=1.50e-04 time=12.0s\n",
            "[ViT_heads=4] Epoch 11: train_loss=1.6399 acc=0.4954 | val_loss=1.7271 acc=0.4713 | lr=1.27e-04 time=12.2s\n",
            "[ViT_heads=4] Epoch 12: train_loss=1.5888 acc=0.5133 | val_loss=1.7457 acc=0.4760 | lr=1.04e-04 time=11.8s\n",
            "[ViT_heads=4] Epoch 13: train_loss=1.5390 acc=0.5247 | val_loss=1.7054 acc=0.4873 | lr=8.19e-05 time=11.3s\n",
            "[ViT_heads=4] Epoch 14: train_loss=1.4992 acc=0.5388 | val_loss=1.6983 acc=0.4900 | lr=6.18e-05 time=11.8s\n",
            "[ViT_heads=4] Epoch 15: train_loss=1.4627 acc=0.5496 | val_loss=1.6690 acc=0.5040 | lr=4.39e-05 time=11.9s\n",
            "[ViT_heads=4] Epoch 16: train_loss=1.4203 acc=0.5634 | val_loss=1.6609 acc=0.5053 | lr=2.86e-05 time=12.2s\n",
            "[ViT_heads=4] Epoch 17: train_loss=1.3904 acc=0.5807 | val_loss=1.6646 acc=0.5047 | lr=1.63e-05 time=12.1s\n",
            "[ViT_heads=4] Epoch 18: train_loss=1.3616 acc=0.5820 | val_loss=1.6515 acc=0.5040 | lr=7.34e-06 time=12.1s\n",
            "[ViT_heads=4] Epoch 19: train_loss=1.3557 acc=0.5900 | val_loss=1.6465 acc=0.5087 | lr=1.85e-06 time=12.1s\n",
            "[ViT_heads=4] Epoch 20: train_loss=1.3493 acc=0.5896 | val_loss=1.6455 acc=0.5067 | lr=0.00e+00 time=12.0s\n",
            "Saved: tri_compare_vit/curves/ViT_heads=4.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_heads=4.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_heads=6 | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 6, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_heads=6] Epoch 01: train_loss=2.6668 acc=0.1726 | val_loss=2.4655 acc=0.2267 | lr=2.98e-04 time=12.2s\n",
            "[ViT_heads=6] Epoch 02: train_loss=2.4220 acc=0.2469 | val_loss=2.2570 acc=0.3020 | lr=2.93e-04 time=12.0s\n",
            "[ViT_heads=6] Epoch 03: train_loss=2.2721 acc=0.2968 | val_loss=2.1247 acc=0.3467 | lr=2.84e-04 time=11.9s\n",
            "[ViT_heads=6] Epoch 04: train_loss=2.1470 acc=0.3299 | val_loss=2.0004 acc=0.3933 | lr=2.71e-04 time=12.2s\n",
            "[ViT_heads=6] Epoch 05: train_loss=2.0429 acc=0.3705 | val_loss=1.9971 acc=0.3820 | lr=2.56e-04 time=12.1s\n",
            "[ViT_heads=6] Epoch 06: train_loss=1.9656 acc=0.3947 | val_loss=1.8787 acc=0.4340 | lr=2.38e-04 time=12.2s\n",
            "[ViT_heads=6] Epoch 07: train_loss=1.8875 acc=0.4127 | val_loss=1.9292 acc=0.4120 | lr=2.18e-04 time=12.1s\n",
            "[ViT_heads=6] Epoch 08: train_loss=1.8215 acc=0.4387 | val_loss=1.8562 acc=0.4520 | lr=1.96e-04 time=12.2s\n",
            "[ViT_heads=6] Epoch 09: train_loss=1.7637 acc=0.4545 | val_loss=1.8131 acc=0.4553 | lr=1.73e-04 time=12.2s\n",
            "[ViT_heads=6] Epoch 10: train_loss=1.6845 acc=0.4838 | val_loss=1.7721 acc=0.4667 | lr=1.50e-04 time=12.0s\n",
            "[ViT_heads=6] Epoch 11: train_loss=1.6276 acc=0.5035 | val_loss=1.7443 acc=0.4880 | lr=1.27e-04 time=11.8s\n",
            "[ViT_heads=6] Epoch 12: train_loss=1.5796 acc=0.5129 | val_loss=1.7224 acc=0.4813 | lr=1.04e-04 time=11.7s\n",
            "[ViT_heads=6] Epoch 13: train_loss=1.5278 acc=0.5312 | val_loss=1.7033 acc=0.4927 | lr=8.19e-05 time=11.7s\n",
            "[ViT_heads=6] Epoch 14: train_loss=1.4760 acc=0.5476 | val_loss=1.6979 acc=0.4920 | lr=6.18e-05 time=11.9s\n",
            "[ViT_heads=6] Epoch 15: train_loss=1.4367 acc=0.5619 | val_loss=1.6789 acc=0.5080 | lr=4.39e-05 time=12.0s\n",
            "[ViT_heads=6] Epoch 16: train_loss=1.4061 acc=0.5732 | val_loss=1.6696 acc=0.5020 | lr=2.86e-05 time=12.3s\n",
            "[ViT_heads=6] Epoch 17: train_loss=1.3777 acc=0.5794 | val_loss=1.6779 acc=0.5047 | lr=1.63e-05 time=12.0s\n",
            "[ViT_heads=6] Epoch 18: train_loss=1.3494 acc=0.5935 | val_loss=1.6628 acc=0.5073 | lr=7.34e-06 time=12.2s\n",
            "[ViT_heads=6] Epoch 19: train_loss=1.3432 acc=0.5926 | val_loss=1.6592 acc=0.5087 | lr=1.85e-06 time=12.2s\n",
            "[ViT_heads=6] Epoch 20: train_loss=1.3367 acc=0.5965 | val_loss=1.6581 acc=0.5120 | lr=0.00e+00 time=12.0s\n",
            "Saved: tri_compare_vit/curves/ViT_heads=6.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_heads=6.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_heads=8 | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 8, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_heads=8] Epoch 01: train_loss=2.6507 acc=0.1799 | val_loss=2.4226 acc=0.2513 | lr=2.98e-04 time=12.2s\n",
            "[ViT_heads=8] Epoch 02: train_loss=2.3858 acc=0.2585 | val_loss=2.2288 acc=0.3120 | lr=2.93e-04 time=12.0s\n",
            "[ViT_heads=8] Epoch 03: train_loss=2.2481 acc=0.3052 | val_loss=2.1240 acc=0.3493 | lr=2.84e-04 time=12.1s\n",
            "[ViT_heads=8] Epoch 04: train_loss=2.1272 acc=0.3408 | val_loss=2.0411 acc=0.3753 | lr=2.71e-04 time=12.2s\n",
            "[ViT_heads=8] Epoch 05: train_loss=2.0259 acc=0.3719 | val_loss=1.9436 acc=0.4013 | lr=2.56e-04 time=12.0s\n",
            "[ViT_heads=8] Epoch 06: train_loss=1.9429 acc=0.4022 | val_loss=1.8752 acc=0.4247 | lr=2.38e-04 time=12.1s\n",
            "[ViT_heads=8] Epoch 07: train_loss=1.8563 acc=0.4226 | val_loss=1.7970 acc=0.4513 | lr=2.18e-04 time=12.0s\n",
            "[ViT_heads=8] Epoch 08: train_loss=1.7786 acc=0.4484 | val_loss=1.7737 acc=0.4647 | lr=1.96e-04 time=11.9s\n",
            "[ViT_heads=8] Epoch 09: train_loss=1.7168 acc=0.4740 | val_loss=1.7577 acc=0.4653 | lr=1.73e-04 time=12.0s\n",
            "[ViT_heads=8] Epoch 10: train_loss=1.6657 acc=0.4849 | val_loss=1.6818 acc=0.4940 | lr=1.50e-04 time=12.0s\n",
            "[ViT_heads=8] Epoch 11: train_loss=1.5908 acc=0.5152 | val_loss=1.6867 acc=0.4893 | lr=1.27e-04 time=11.7s\n",
            "[ViT_heads=8] Epoch 12: train_loss=1.5409 acc=0.5266 | val_loss=1.6540 acc=0.4933 | lr=1.04e-04 time=11.7s\n",
            "[ViT_heads=8] Epoch 13: train_loss=1.4930 acc=0.5399 | val_loss=1.6521 acc=0.5007 | lr=8.19e-05 time=11.6s\n",
            "[ViT_heads=8] Epoch 14: train_loss=1.4517 acc=0.5526 | val_loss=1.6242 acc=0.5120 | lr=6.18e-05 time=12.2s\n",
            "[ViT_heads=8] Epoch 15: train_loss=1.3993 acc=0.5762 | val_loss=1.6304 acc=0.5093 | lr=4.39e-05 time=12.0s\n",
            "[ViT_heads=8] Epoch 16: train_loss=1.3755 acc=0.5804 | val_loss=1.5989 acc=0.5100 | lr=2.86e-05 time=12.1s\n",
            "[ViT_heads=8] Epoch 17: train_loss=1.3487 acc=0.5882 | val_loss=1.6068 acc=0.5167 | lr=1.63e-05 time=12.1s\n",
            "[ViT_heads=8] Epoch 18: train_loss=1.3318 acc=0.5938 | val_loss=1.5986 acc=0.5120 | lr=7.34e-06 time=12.0s\n",
            "[ViT_heads=8] Epoch 19: train_loss=1.3225 acc=0.5938 | val_loss=1.5918 acc=0.5127 | lr=1.85e-06 time=12.2s\n",
            "[ViT_heads=8] Epoch 20: train_loss=1.3083 acc=0.5952 | val_loss=1.5909 acc=0.5127 | lr=0.00e+00 time=12.1s\n",
            "Saved: tri_compare_vit/curves/ViT_heads=8.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_heads=8.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_PatchConv | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_PatchConv] Epoch 01: train_loss=2.6515 acc=0.1819 | val_loss=2.4346 acc=0.2533 | lr=2.98e-04 time=12.1s\n",
            "[ViT_PatchConv] Epoch 02: train_loss=2.4006 acc=0.2556 | val_loss=2.2250 acc=0.3313 | lr=2.93e-04 time=11.9s\n",
            "[ViT_PatchConv] Epoch 03: train_loss=2.2569 acc=0.3000 | val_loss=2.1057 acc=0.3660 | lr=2.84e-04 time=12.0s\n",
            "[ViT_PatchConv] Epoch 04: train_loss=2.1488 acc=0.3349 | val_loss=2.0319 acc=0.3853 | lr=2.71e-04 time=12.1s\n",
            "[ViT_PatchConv] Epoch 05: train_loss=2.0677 acc=0.3678 | val_loss=1.9839 acc=0.4020 | lr=2.56e-04 time=12.0s\n",
            "[ViT_PatchConv] Epoch 06: train_loss=1.9885 acc=0.3896 | val_loss=1.9494 acc=0.4027 | lr=2.38e-04 time=12.0s\n",
            "[ViT_PatchConv] Epoch 07: train_loss=1.9292 acc=0.4056 | val_loss=1.9159 acc=0.4187 | lr=2.18e-04 time=11.8s\n",
            "[ViT_PatchConv] Epoch 08: train_loss=1.8578 acc=0.4302 | val_loss=1.8348 acc=0.4447 | lr=1.96e-04 time=11.9s\n",
            "[ViT_PatchConv] Epoch 09: train_loss=1.7932 acc=0.4548 | val_loss=1.8096 acc=0.4420 | lr=1.73e-04 time=11.7s\n",
            "[ViT_PatchConv] Epoch 10: train_loss=1.7304 acc=0.4631 | val_loss=1.7682 acc=0.4673 | lr=1.50e-04 time=11.3s\n",
            "[ViT_PatchConv] Epoch 11: train_loss=1.6672 acc=0.4896 | val_loss=1.7343 acc=0.4707 | lr=1.27e-04 time=11.8s\n",
            "[ViT_PatchConv] Epoch 12: train_loss=1.6170 acc=0.5041 | val_loss=1.7374 acc=0.4720 | lr=1.04e-04 time=12.0s\n",
            "[ViT_PatchConv] Epoch 13: train_loss=1.5813 acc=0.5125 | val_loss=1.7139 acc=0.4900 | lr=8.19e-05 time=12.0s\n",
            "[ViT_PatchConv] Epoch 14: train_loss=1.5282 acc=0.5329 | val_loss=1.7010 acc=0.4820 | lr=6.18e-05 time=12.0s\n",
            "[ViT_PatchConv] Epoch 15: train_loss=1.4980 acc=0.5425 | val_loss=1.6683 acc=0.5000 | lr=4.39e-05 time=11.9s\n",
            "[ViT_PatchConv] Epoch 16: train_loss=1.4590 acc=0.5535 | val_loss=1.6757 acc=0.4900 | lr=2.86e-05 time=12.1s\n",
            "[ViT_PatchConv] Epoch 17: train_loss=1.4353 acc=0.5604 | val_loss=1.6521 acc=0.5013 | lr=1.63e-05 time=12.0s\n",
            "[ViT_PatchConv] Epoch 18: train_loss=1.4124 acc=0.5671 | val_loss=1.6523 acc=0.4987 | lr=7.34e-06 time=11.9s\n",
            "[ViT_PatchConv] Epoch 19: train_loss=1.3909 acc=0.5800 | val_loss=1.6443 acc=0.5033 | lr=1.85e-06 time=12.2s\n",
            "[ViT_PatchConv] Epoch 20: train_loss=1.3872 acc=0.5773 | val_loss=1.6426 acc=0.5007 | lr=0.00e+00 time=12.1s\n",
            "Saved: tri_compare_vit/curves/ViT_PatchConv.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_PatchConv.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_PatchLinear | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'linear', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_PatchLinear] Epoch 01: train_loss=2.6534 acc=0.1774 | val_loss=2.4376 acc=0.2480 | lr=2.98e-04 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 02: train_loss=2.4081 acc=0.2631 | val_loss=2.2341 acc=0.3200 | lr=2.93e-04 time=12.0s\n",
            "[ViT_PatchLinear] Epoch 03: train_loss=2.2390 acc=0.3065 | val_loss=2.1317 acc=0.3533 | lr=2.84e-04 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 04: train_loss=2.1333 acc=0.3422 | val_loss=2.0288 acc=0.3973 | lr=2.71e-04 time=12.0s\n",
            "[ViT_PatchLinear] Epoch 05: train_loss=2.0449 acc=0.3682 | val_loss=2.0501 acc=0.3860 | lr=2.56e-04 time=11.9s\n",
            "[ViT_PatchLinear] Epoch 06: train_loss=1.9545 acc=0.4007 | val_loss=1.9421 acc=0.4167 | lr=2.38e-04 time=11.9s\n",
            "[ViT_PatchLinear] Epoch 07: train_loss=1.8830 acc=0.4211 | val_loss=1.8838 acc=0.4347 | lr=2.18e-04 time=11.6s\n",
            "[ViT_PatchLinear] Epoch 08: train_loss=1.8242 acc=0.4292 | val_loss=1.8564 acc=0.4293 | lr=1.96e-04 time=11.5s\n",
            "[ViT_PatchLinear] Epoch 09: train_loss=1.7768 acc=0.4474 | val_loss=1.8319 acc=0.4453 | lr=1.73e-04 time=11.9s\n",
            "[ViT_PatchLinear] Epoch 10: train_loss=1.7052 acc=0.4796 | val_loss=1.8051 acc=0.4420 | lr=1.50e-04 time=12.2s\n",
            "[ViT_PatchLinear] Epoch 11: train_loss=1.6531 acc=0.4865 | val_loss=1.7575 acc=0.4773 | lr=1.27e-04 time=12.0s\n",
            "[ViT_PatchLinear] Epoch 12: train_loss=1.5899 acc=0.5105 | val_loss=1.7815 acc=0.4513 | lr=1.04e-04 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 13: train_loss=1.5497 acc=0.5226 | val_loss=1.7180 acc=0.4807 | lr=8.19e-05 time=12.0s\n",
            "[ViT_PatchLinear] Epoch 14: train_loss=1.4974 acc=0.5474 | val_loss=1.7084 acc=0.4893 | lr=6.18e-05 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 15: train_loss=1.4644 acc=0.5495 | val_loss=1.7091 acc=0.4847 | lr=4.39e-05 time=12.0s\n",
            "[ViT_PatchLinear] Epoch 16: train_loss=1.4308 acc=0.5645 | val_loss=1.6925 acc=0.4907 | lr=2.86e-05 time=12.2s\n",
            "[ViT_PatchLinear] Epoch 17: train_loss=1.4118 acc=0.5706 | val_loss=1.6886 acc=0.4887 | lr=1.63e-05 time=11.9s\n",
            "[ViT_PatchLinear] Epoch 18: train_loss=1.3846 acc=0.5806 | val_loss=1.6788 acc=0.4953 | lr=7.34e-06 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 19: train_loss=1.3719 acc=0.5913 | val_loss=1.6736 acc=0.4933 | lr=1.85e-06 time=12.1s\n",
            "[ViT_PatchLinear] Epoch 20: train_loss=1.3518 acc=0.5928 | val_loss=1.6740 acc=0.4933 | lr=0.00e+00 time=12.0s\n",
            "Saved: tri_compare_vit/curves/ViT_PatchLinear.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_PatchLinear.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_PosLearn | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'learnable'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_PosLearn] Epoch 01: train_loss=2.6638 acc=0.1741 | val_loss=2.4684 acc=0.2087 | lr=2.98e-04 time=12.0s\n",
            "[ViT_PosLearn] Epoch 02: train_loss=2.4321 acc=0.2501 | val_loss=2.2959 acc=0.2660 | lr=2.93e-04 time=11.9s\n",
            "[ViT_PosLearn] Epoch 03: train_loss=2.2853 acc=0.2931 | val_loss=2.1665 acc=0.3353 | lr=2.84e-04 time=12.0s\n",
            "[ViT_PosLearn] Epoch 04: train_loss=2.1611 acc=0.3381 | val_loss=2.0354 acc=0.3793 | lr=2.71e-04 time=11.9s\n",
            "[ViT_PosLearn] Epoch 05: train_loss=2.0500 acc=0.3673 | val_loss=1.9773 acc=0.4140 | lr=2.56e-04 time=11.9s\n",
            "[ViT_PosLearn] Epoch 06: train_loss=1.9716 acc=0.3938 | val_loss=1.9068 acc=0.4267 | lr=2.38e-04 time=11.4s\n",
            "[ViT_PosLearn] Epoch 07: train_loss=1.8936 acc=0.4175 | val_loss=1.8773 acc=0.4307 | lr=2.18e-04 time=11.8s\n",
            "[ViT_PosLearn] Epoch 08: train_loss=1.8227 acc=0.4346 | val_loss=1.8413 acc=0.4447 | lr=1.96e-04 time=12.2s\n",
            "[ViT_PosLearn] Epoch 09: train_loss=1.7677 acc=0.4533 | val_loss=1.7789 acc=0.4593 | lr=1.73e-04 time=12.0s\n",
            "[ViT_PosLearn] Epoch 10: train_loss=1.6916 acc=0.4774 | val_loss=1.7458 acc=0.4647 | lr=1.50e-04 time=11.8s\n",
            "[ViT_PosLearn] Epoch 11: train_loss=1.6383 acc=0.5002 | val_loss=1.7427 acc=0.4833 | lr=1.27e-04 time=11.9s\n",
            "[ViT_PosLearn] Epoch 12: train_loss=1.5773 acc=0.5156 | val_loss=1.7054 acc=0.4800 | lr=1.04e-04 time=12.0s\n",
            "[ViT_PosLearn] Epoch 13: train_loss=1.5288 acc=0.5296 | val_loss=1.6746 acc=0.4880 | lr=8.19e-05 time=12.0s\n",
            "[ViT_PosLearn] Epoch 14: train_loss=1.4850 acc=0.5478 | val_loss=1.6540 acc=0.4927 | lr=6.18e-05 time=12.1s\n",
            "[ViT_PosLearn] Epoch 15: train_loss=1.4504 acc=0.5588 | val_loss=1.6589 acc=0.5073 | lr=4.39e-05 time=12.0s\n",
            "[ViT_PosLearn] Epoch 16: train_loss=1.4205 acc=0.5700 | val_loss=1.6390 acc=0.5000 | lr=2.86e-05 time=12.0s\n",
            "[ViT_PosLearn] Epoch 17: train_loss=1.3935 acc=0.5728 | val_loss=1.6396 acc=0.5133 | lr=1.63e-05 time=12.1s\n",
            "[ViT_PosLearn] Epoch 18: train_loss=1.3627 acc=0.5864 | val_loss=1.6184 acc=0.5007 | lr=7.34e-06 time=12.4s\n",
            "[ViT_PosLearn] Epoch 19: train_loss=1.3558 acc=0.5849 | val_loss=1.6197 acc=0.5060 | lr=1.85e-06 time=11.9s\n",
            "[ViT_PosLearn] Epoch 20: train_loss=1.3346 acc=0.5956 | val_loss=1.6186 acc=0.5060 | lr=0.00e+00 time=11.6s\n",
            "Saved: tri_compare_vit/curves/ViT_PosLearn.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_PosLearn.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_PosSine | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'sinusoidal'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_PosSine] Epoch 01: train_loss=2.7579 acc=0.1495 | val_loss=2.5880 acc=0.2040 | lr=2.98e-04 time=12.2s\n",
            "[ViT_PosSine] Epoch 02: train_loss=2.5562 acc=0.2066 | val_loss=2.3394 acc=0.2747 | lr=2.93e-04 time=11.9s\n",
            "[ViT_PosSine] Epoch 03: train_loss=2.3580 acc=0.2776 | val_loss=2.1427 acc=0.3507 | lr=2.84e-04 time=11.5s\n",
            "[ViT_PosSine] Epoch 04: train_loss=2.1762 acc=0.3335 | val_loss=1.9881 acc=0.3933 | lr=2.71e-04 time=11.6s\n",
            "[ViT_PosSine] Epoch 05: train_loss=2.0465 acc=0.3686 | val_loss=1.9252 acc=0.4027 | lr=2.56e-04 time=11.8s\n",
            "[ViT_PosSine] Epoch 06: train_loss=1.9469 acc=0.4020 | val_loss=1.8733 acc=0.4167 | lr=2.38e-04 time=12.1s\n",
            "[ViT_PosSine] Epoch 07: train_loss=1.8679 acc=0.4232 | val_loss=1.8243 acc=0.4360 | lr=2.18e-04 time=12.2s\n",
            "[ViT_PosSine] Epoch 08: train_loss=1.7934 acc=0.4476 | val_loss=1.8120 acc=0.4513 | lr=1.96e-04 time=12.0s\n",
            "[ViT_PosSine] Epoch 09: train_loss=1.7222 acc=0.4747 | val_loss=1.7549 acc=0.4640 | lr=1.73e-04 time=12.1s\n",
            "[ViT_PosSine] Epoch 10: train_loss=1.6587 acc=0.4946 | val_loss=1.7122 acc=0.4720 | lr=1.50e-04 time=12.0s\n",
            "[ViT_PosSine] Epoch 11: train_loss=1.5980 acc=0.5115 | val_loss=1.7037 acc=0.4860 | lr=1.27e-04 time=12.0s\n",
            "[ViT_PosSine] Epoch 12: train_loss=1.5444 acc=0.5321 | val_loss=1.6645 acc=0.5000 | lr=1.04e-04 time=12.2s\n",
            "[ViT_PosSine] Epoch 13: train_loss=1.4988 acc=0.5456 | val_loss=1.6415 acc=0.5093 | lr=8.19e-05 time=12.0s\n",
            "[ViT_PosSine] Epoch 14: train_loss=1.4487 acc=0.5562 | val_loss=1.6312 acc=0.5067 | lr=6.18e-05 time=12.1s\n",
            "[ViT_PosSine] Epoch 15: train_loss=1.4052 acc=0.5767 | val_loss=1.6105 acc=0.5160 | lr=4.39e-05 time=12.0s\n",
            "[ViT_PosSine] Epoch 16: train_loss=1.3713 acc=0.5848 | val_loss=1.5972 acc=0.5220 | lr=2.86e-05 time=11.8s\n",
            "[ViT_PosSine] Epoch 17: train_loss=1.3391 acc=0.5976 | val_loss=1.5982 acc=0.5067 | lr=1.63e-05 time=11.9s\n",
            "[ViT_PosSine] Epoch 18: train_loss=1.3232 acc=0.5991 | val_loss=1.5880 acc=0.5227 | lr=7.34e-06 time=12.0s\n",
            "[ViT_PosSine] Epoch 19: train_loss=1.3115 acc=0.6102 | val_loss=1.5841 acc=0.5187 | lr=1.85e-06 time=11.9s\n",
            "[ViT_PosSine] Epoch 20: train_loss=1.3044 acc=0.6078 | val_loss=1.5813 acc=0.5273 | lr=0.00e+00 time=11.4s\n",
            "Saved: tri_compare_vit/curves/ViT_PosSine.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_PosSine.png\n",
            "\n",
            "================================================================================\n",
            "Training: ViT_PosNone | ViT {'patch': 8, 'embed_dim': 192, 'depth': 6, 'heads': 4, 'patch_type': 'conv', 'pos_type': 'none'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2861665181.py:338: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(AMP and device.startswith(\"cuda\")))\n",
            "/tmp/ipython-input-2861665181.py:350: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(AMP and device.startswith(\"cuda\"))):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ViT_PosNone] Epoch 01: train_loss=2.6679 acc=0.1741 | val_loss=2.4524 acc=0.2440 | lr=2.98e-04 time=11.5s\n",
            "[ViT_PosNone] Epoch 02: train_loss=2.4476 acc=0.2467 | val_loss=2.2584 acc=0.3173 | lr=2.93e-04 time=11.4s\n",
            "[ViT_PosNone] Epoch 03: train_loss=2.2751 acc=0.3005 | val_loss=2.1071 acc=0.3687 | lr=2.84e-04 time=11.7s\n",
            "[ViT_PosNone] Epoch 04: train_loss=2.1642 acc=0.3278 | val_loss=2.0627 acc=0.3680 | lr=2.71e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 05: train_loss=2.0661 acc=0.3602 | val_loss=1.9585 acc=0.4213 | lr=2.56e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 06: train_loss=1.9707 acc=0.3888 | val_loss=1.9140 acc=0.4247 | lr=2.38e-04 time=12.0s\n",
            "[ViT_PosNone] Epoch 07: train_loss=1.9055 acc=0.4162 | val_loss=1.8858 acc=0.4340 | lr=2.18e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 08: train_loss=1.8332 acc=0.4325 | val_loss=1.8094 acc=0.4587 | lr=1.96e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 09: train_loss=1.7848 acc=0.4474 | val_loss=1.7955 acc=0.4627 | lr=1.73e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 10: train_loss=1.7126 acc=0.4728 | val_loss=1.7780 acc=0.4800 | lr=1.50e-04 time=12.0s\n",
            "[ViT_PosNone] Epoch 11: train_loss=1.6703 acc=0.4842 | val_loss=1.7508 acc=0.4860 | lr=1.27e-04 time=11.9s\n",
            "[ViT_PosNone] Epoch 12: train_loss=1.6113 acc=0.5000 | val_loss=1.7551 acc=0.4787 | lr=1.04e-04 time=12.1s\n",
            "[ViT_PosNone] Epoch 13: train_loss=1.5715 acc=0.5184 | val_loss=1.7370 acc=0.4693 | lr=8.19e-05 time=12.1s\n",
            "[ViT_PosNone] Epoch 14: train_loss=1.5279 acc=0.5306 | val_loss=1.7236 acc=0.4953 | lr=6.18e-05 time=12.1s\n",
            "[ViT_PosNone] Epoch 15: train_loss=1.4861 acc=0.5496 | val_loss=1.7015 acc=0.4933 | lr=4.39e-05 time=12.0s\n",
            "[ViT_PosNone] Epoch 16: train_loss=1.4583 acc=0.5504 | val_loss=1.6973 acc=0.4980 | lr=2.86e-05 time=11.8s\n",
            "[ViT_PosNone] Epoch 17: train_loss=1.4289 acc=0.5655 | val_loss=1.6843 acc=0.5080 | lr=1.63e-05 time=11.5s\n",
            "[ViT_PosNone] Epoch 18: train_loss=1.3973 acc=0.5678 | val_loss=1.6840 acc=0.5060 | lr=7.34e-06 time=11.6s\n",
            "[ViT_PosNone] Epoch 19: train_loss=1.3973 acc=0.5719 | val_loss=1.6793 acc=0.5093 | lr=1.85e-06 time=11.9s\n",
            "[ViT_PosNone] Epoch 20: train_loss=1.3787 acc=0.5816 | val_loss=1.6771 acc=0.5120 | lr=0.00e+00 time=12.1s\n",
            "Saved: tri_compare_vit/curves/ViT_PosNone.png\n",
            "Saved: tri_compare_vit/figs/cm_ViT_PosNone.png\n",
            "\n",
            "Saved summary: tri_compare_vit/summary.csv\n",
            "                 name family  val_acc  val_loss\n",
            "             SmallCNN    CNN 0.663333  1.160207\n",
            "ViT(h=4,Conv,LearnPE)    ViT 0.530000  1.620059\n",
            "          ViT_PosSine    ViT 0.527333  1.581337\n",
            "          ViT_heads=2    ViT 0.518000  1.633325\n",
            "          ViT_heads=8    ViT 0.512667  1.590942\n",
            "          ViT_heads=6    ViT 0.512000  1.658118\n",
            "          ViT_PosNone    ViT 0.512000  1.677102\n",
            "          ViT_heads=4    ViT 0.506667  1.645499\n",
            "         ViT_PosLearn    ViT 0.500667  1.618436\n",
            "        ViT_PatchConv    ViT 0.500667  1.642643\n",
            "      ViT_PatchLinear    ViT 0.493333  1.673637\n",
            "                FCFNN  FCFNN 0.363333  2.159235\n",
            "Saved: tri_compare_vit/figs/best_per_family.png\n",
            "Saved: tri_compare_vit/figs/vit_heads_ablation.png\n",
            "Saved: tri_compare_vit/figs/vit_patch_ablation.png\n",
            "Saved: tri_compare_vit/figs/vit_pos_ablation.png\n",
            "Saved: tri_compare_vit/figs/combined_table.png\n",
            "Saved: tri_compare_vit/figs/dashboard.png\n",
            "Saved: tri_compare_vit/figs/best_family_curves.png\n",
            "Saved PDF report: tri_compare_vit/figs/report.pdf\n"
          ]
        }
      ]
    }
  ]
}