{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b2f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac1e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images(src_folder, dst_folder, max_images=10000):\n",
    "    count = 0\n",
    "    files = sorted(os.listdir(src_folder))\n",
    "    for f in files:\n",
    "        src_path = os.path.join(src_folder, f)\n",
    "        if os.path.isfile(src_path) and os.path.splitext(f)[1].lower() in valid_exts:\n",
    "            shutil.copy(src_path, os.path.join(dst_folder, f))\n",
    "            count += 1\n",
    "            if count >= max_images:\n",
    "                break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3de154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files_A = sorted([p for p in Path(root, \"trainA\").glob(\"*\") if p.suffix.lower() in valid_exts])\n",
    "        self.files_B = sorted([p for p in Path(root, \"trainB\").glob(\"*\") if p.suffix.lower() in valid_exts])\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.files_A), len(self.files_B))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_A = Image.open(self.files_A[idx]).convert(\"RGB\")\n",
    "        img_B = Image.open(self.files_B[idx]).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1999e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim, dim, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a382e7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, n_res_blocks=6):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_nc, 64, 7, 1, 3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 2, 1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 2, 1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        for _ in range(n_res_blocks):\n",
    "            layers.append(ResnetBlock(256))\n",
    "\n",
    "        layers += [\n",
    "            nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, out_nc, 7, 1, 3),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2b5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_nc):\n",
    "        super().__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_nc, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 1, 1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 1),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f108aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    return x * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c75353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if CelebA folder exists: True\n",
      "Checking if Portrait folder exists: True\n",
      "Copied 3144 images to cyclegan_faces\\trainA\n",
      "Copied 3144 images to cyclegan_faces\\trainB\n",
      "Final counts:\n",
      "trainA images: 3144\n",
      "trainB images: 3144\n",
      "Dataset length: 3144\n",
      "cuda\n",
      "Epoch 1 | G Loss: 5.3666 | D_A Loss: 0.1649 | D_B Loss: 0.0441\n",
      "Epoch 2 | G Loss: 5.6223 | D_A Loss: 0.0664 | D_B Loss: 0.1780\n",
      "Epoch 3 | G Loss: 3.7785 | D_A Loss: 0.1074 | D_B Loss: 0.2073\n",
      "Epoch 4 | G Loss: 3.6984 | D_A Loss: 0.1075 | D_B Loss: 0.0502\n",
      "Epoch 5 | G Loss: 3.9348 | D_A Loss: 0.1659 | D_B Loss: 0.2188\n",
      "Epoch 6 | G Loss: 4.6372 | D_A Loss: 0.0601 | D_B Loss: 0.1366\n",
      "Epoch 7 | G Loss: 3.1381 | D_A Loss: 0.1280 | D_B Loss: 0.1280\n",
      "Epoch 8 | G Loss: 2.7244 | D_A Loss: 0.1443 | D_B Loss: 0.2261\n",
      "Epoch 9 | G Loss: 4.1183 | D_A Loss: 0.1160 | D_B Loss: 0.2146\n",
      "Epoch 10 | G Loss: 3.6623 | D_A Loss: 0.2489 | D_B Loss: 0.2617\n",
      "Epoch 11 | G Loss: 3.5102 | D_A Loss: 0.0424 | D_B Loss: 0.0742\n",
      "Epoch 12 | G Loss: 2.8221 | D_A Loss: 0.0577 | D_B Loss: 0.1780\n",
      "Epoch 13 | G Loss: 5.1417 | D_A Loss: 0.0249 | D_B Loss: 0.1751\n",
      "Epoch 14 | G Loss: 4.4514 | D_A Loss: 0.0310 | D_B Loss: 0.2669\n",
      "Epoch 15 | G Loss: 5.3327 | D_A Loss: 0.0942 | D_B Loss: 0.1926\n",
      "Epoch 16 | G Loss: 5.9619 | D_A Loss: 0.1253 | D_B Loss: 0.1402\n",
      "Epoch 17 | G Loss: 4.0855 | D_A Loss: 0.0936 | D_B Loss: 0.0520\n",
      "Epoch 18 | G Loss: 5.0643 | D_A Loss: 0.0450 | D_B Loss: 0.0571\n",
      "Epoch 19 | G Loss: 3.7007 | D_A Loss: 0.0408 | D_B Loss: 0.0146\n",
      "Epoch 20 | G Loss: 3.5775 | D_A Loss: 0.2236 | D_B Loss: 0.0478\n",
      "Epoch 21 | G Loss: 3.0512 | D_A Loss: 0.1225 | D_B Loss: 0.0645\n",
      "Epoch 22 | G Loss: 2.9904 | D_A Loss: 0.1368 | D_B Loss: 0.0361\n",
      "Epoch 23 | G Loss: 3.4636 | D_A Loss: 0.0686 | D_B Loss: 0.0641\n",
      "Epoch 24 | G Loss: 3.6628 | D_A Loss: 0.0538 | D_B Loss: 0.0852\n",
      "Epoch 25 | G Loss: 2.6949 | D_A Loss: 0.1379 | D_B Loss: 0.0662\n"
     ]
    }
   ],
   "source": [
    "celeba_img_folder = r\"D:\\sangita-mam\\assginment-13\\img_align_celeba\\img_align_celeba\"\n",
    "portrait_img_folder = r\"C:\\Users\\VRLAB02\\OneDrive\\Desktop\\data\\paintings\"\n",
    "\n",
    "print(\"Checking if CelebA folder exists:\", os.path.exists(celeba_img_folder))\n",
    "print(\"Checking if Portrait folder exists:\", os.path.exists(portrait_img_folder))\n",
    "\n",
    "\n",
    "root_dir = \"cyclegan_faces\"\n",
    "trainA_dir = os.path.join(root_dir, \"trainA\")\n",
    "trainB_dir = os.path.join(root_dir, \"trainB\")\n",
    "os.makedirs(trainA_dir, exist_ok=True)\n",
    "os.makedirs(trainB_dir, exist_ok=True)\n",
    "\n",
    "valid_exts = [\".jpg\", \".jpeg\", \".png\"]\n",
    "numA = copy_images(celeba_img_folder, trainA_dir, 3144)\n",
    "numB = copy_images(portrait_img_folder, trainB_dir, 3144)\n",
    "\n",
    "print(f\"Copied {numA} images to {trainA_dir}\")\n",
    "print(f\"Copied {numB} images to {trainB_dir}\")\n",
    "\n",
    "print(\"Final counts:\")\n",
    "print(\"trainA images:\", len(os.listdir(trainA_dir)))\n",
    "print(\"trainB images:\", len(os.listdir(trainB_dir)))\n",
    "\n",
    "# Define Dataset class\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dataset = ImageDataset(root_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "print(f\"Dataset length: {len(dataset)}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "G_AB = Generator(3, 3).to(device)\n",
    "G_BA = Generator(3, 3).to(device)\n",
    "D_A = Discriminator(3).to(device)\n",
    "D_B = Discriminator(3).to(device)\n",
    "\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "\n",
    "optim_G = optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D_A = optim.Adam(D_A.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "optim_D_B = optim.Adam(D_B.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for batch in dataloader:\n",
    "        real_A = batch[\"A\"].to(device)\n",
    "        real_B = batch[\"B\"].to(device)\n",
    "\n",
    "        # Train Generators\n",
    "        optim_G.zero_grad()\n",
    "\n",
    "        fake_B = G_AB(real_A)\n",
    "        fake_A = G_BA(real_B)\n",
    "\n",
    "        loss_GAN_AB = criterion_GAN(D_B(fake_B), torch.ones_like(D_B(fake_B)))\n",
    "        loss_GAN_BA = criterion_GAN(D_A(fake_A), torch.ones_like(D_A(fake_A)))\n",
    "\n",
    "        recov_A = G_BA(fake_B)\n",
    "        recov_B = G_AB(fake_A)\n",
    "\n",
    "        loss_cycle_A = criterion_cycle(recov_A, real_A)\n",
    "        loss_cycle_B = criterion_cycle(recov_B, real_B)\n",
    "\n",
    "        loss_G = loss_GAN_AB + loss_GAN_BA + 10 * (loss_cycle_A + loss_cycle_B)\n",
    "        loss_G.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "        # Train Discriminator A\n",
    "        optim_D_A.zero_grad()\n",
    "        loss_D_A = 0.5 * (criterion_GAN(D_A(real_A), torch.ones_like(D_A(real_A))) +\n",
    "                          criterion_GAN(D_A(fake_A.detach()), torch.zeros_like(D_A(fake_A))))\n",
    "        loss_D_A.backward()\n",
    "        optim_D_A.step()\n",
    "\n",
    "        # Train Discriminator B\n",
    "        optim_D_B.zero_grad()\n",
    "        loss_D_B = 0.5 * (criterion_GAN(D_B(real_B), torch.ones_like(D_B(real_B))) +\n",
    "                          criterion_GAN(D_B(fake_B.detach()), torch.zeros_like(D_B(fake_B))))\n",
    "        loss_D_B.backward()\n",
    "        optim_D_B.step()\n",
    "\n",
    "    print(f\"Epoch {epoch} | G Loss: {loss_G.item():.4f} | D_A Loss: {loss_D_A.item():.4f} | D_B Loss: {loss_D_B.item():.4f}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        G_AB.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get 5 real images from dataset\n",
    "            real = torch.stack([dataset[i][\"A\"] for i in range(5)], dim=0).to(device)\n",
    "            fake = G_AB(real).cpu()\n",
    "            real = real.cpu()\n",
    "\n",
    "            real = denorm(real)\n",
    "            fake = denorm(fake)\n",
    "\n",
    "            # Create two rows: one for real, one for fake\n",
    "            real_row = utils.make_grid(real, nrow=5, padding=2)\n",
    "            fake_row = utils.make_grid(fake, nrow=5, padding=2)\n",
    "\n",
    "            # Stack vertically (CHW)\n",
    "            grid = torch.cat([real_row, fake_row], dim=1)  # concat in height\n",
    "\n",
    "            # Plot\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            plt.imshow(grid.permute(1, 2, 0))\n",
    "            plt.axis('off')\n",
    "            plt.title(f'CycleGAN Output - Epoch {epoch}')\n",
    "            plt.savefig(f\"outputs/cyclegan_epoch_{epoch}.png\")\n",
    "            plt.close()\n",
    "        G_AB.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
